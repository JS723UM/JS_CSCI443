{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfb44d7e-e43c-46bf-8320-132e6e6a9325",
   "metadata": {},
   "source": [
    "# CSCI443 HW 5 EDA, Data Cleaning, and Hypothesis Testing.\n",
    "\n",
    "In this homework we perform data cleaning and some analysis of the Olist kaggle data.\n",
    "\n",
    "We wil be using \n",
    "o\n",
    "  https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\n",
    "\n",
    "This dataset spans 100k orders from 2016 and 2018.  We will build a pipeline\n",
    "for running periodic hypothesis tests to detect anomalies.\n",
    "\n",
    "Use Pandas-on-Spark for all problems where appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baebf8c-3c15-43aa-ac50-b3adcbc779f2",
   "metadata": {},
   "source": [
    "## Part I: Upload the Olist Data\n",
    "\n",
    "**Problem 1.1.** Upload the data and ouput a list of the files here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176b1784-5fd2-4f59-90d8-593ed67bd34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57829dc9-4e0e-4fc2-8ff1-6d64c6bebf23",
   "metadata": {},
   "source": [
    "**Problem 1.2.** Load `olist_customers_dataset.csv` into a Pandas-on-Spark DataFrame and output the first ten records using display to pretty-print the DataFrame.\n",
    "\n",
    "   This confirms that we are able to access the data from within Databricks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da73382c-60c5-4522-843e-45f473be1a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59997d7e-e4fd-4f18-8575-a928f9ff13ea",
   "metadata": {},
   "source": [
    "## Part II: Exploratory Data Analysis and Data Cleaning\n",
    "\n",
    "The point of exploratory data analysis is to make sense of the data.\n",
    "\n",
    "Exploratory data analysis encompasses activies like\n",
    "\n",
    "1. Understanding the data structure\n",
    "   *  look at the number of rows and columns to determine the scale of the data.\n",
    "   *  look column labels, value types, and value ranges.\n",
    "   *  identify categorical data and values taken by each kind of categorical data\n",
    "   *  summarize the semantics of each column.\n",
    "\n",
    "2. Summarizing data\n",
    "   * computing summary statistics like means, stddevs, and counts.\n",
    "     \n",
    "3. Identifying missing values.\n",
    "   * counting number of missing values and for which fields.\n",
    "   * This can be used later in determining what kind of data cleaning may be needed.\n",
    "\n",
    "4. Visualizing the data\n",
    "   \n",
    "6. Detecting outliers\n",
    "   * create box plots of each type of numeric data. Using 1.5 IQR rule of thumb to\n",
    "     identify outliers.\n",
    "   * plot geographic data on a map or on a scatter plot to see if is sensical.\n",
    "\n",
    "7. Assessing relationships\n",
    "   * create correlation matrices\n",
    "   * create contingency tables for categorical data\n",
    "   * perform preliminary hypothesis tests, e.g., Chi-squared independence tests.\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c7f1a4-b2d4-4ac5-85d9-bfe90545309c",
   "metadata": {},
   "source": [
    "### Part II.A. Customer Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19325eb7-1ca2-4e0c-9bd0-2e134826db1b",
   "metadata": {},
   "source": [
    "**Problem 2.A.1.** List the size of each data file.  What is the physical memory in your local machine?  Is the aggregate size of the data files larger than the physical memory on your local machine?\n",
    "If the aggregate size of the datasets is larger than your physical memory then this is a good indicator that Pandas is not the right tool.  Pandas typically reads entire DataFrames into physical memory. \n",
    "\n",
    "Note: We will use Databricks regardless of the answer to this question, but it is generally\n",
    "a good first step.  If the data is small, Pandas is often the correct answer.  Most modern laptops have\n",
    "8GB or more of physical memory.  16GB or 3GB is common.   If the dataset size is >10 GB, Spark on a cluster\n",
    "may be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbe941b-8266-4ea0-9198-910d27ad255a",
   "metadata": {},
   "source": [
    "HERE OUTPUT FILE SIZES AND FIND THE SUM OF THE FILE SIZES.\n",
    "LOOKUP THE PHYSICAL MEMORY OF YOUR LAPTOP AND COMPARE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eaee16-bc06-4a90-8ae2-2af8a6b0da98",
   "metadata": {},
   "source": [
    "**Problem 2.A.2.** Let's perform some exploratory data analysis of `olist_customers.csv`.\n",
    "We already loaded it using `read_csv` and we looked at the first few records.\n",
    "List the fields and write one sentence describing what each means.  Read about the\n",
    "data online to gain some understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01165e84-58c9-48a8-9e22-f6d542a1fb7b",
   "metadata": {},
   "source": [
    "HERE INSERT MARKDOWN FOR EACH FIELD AND ITS MEANING."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1fb2a6-f50f-4b81-9f71-b852b5201169",
   "metadata": {},
   "source": [
    "**Problem 2.A.3.** What is the difference between `customer_id` and `customer_unique_id`?  Which would we expect to have more unique IDs?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936eb63e-3166-4194-bc07-84151c21b8a8",
   "metadata": {},
   "source": [
    "HERE INSERT EXPLANATION OF DIFFERENCE AND EXPECTATION OF RELATIVE NUMBER."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c521e4d-f0ff-4609-8825-2a55fe7348ab",
   "metadata": {},
   "source": [
    "**Problem 2.A.4.** How many unique `customer_ids` are there?  How many unique `customer_unique_ids`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009038c9-6d79-4516-8aa4-f5fe6f77fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "HERE INSERT CODE TO COUNT THE NUMBER OF UNIQUE CUSTOMER IDS AND CUSTOMER UNIQUE IDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6b1d5a-b550-459a-9e22-2c0fa3455ff9",
   "metadata": {},
   "source": [
    "**Problem 2.A.5.** Can a customer_unique_id reasonably appear more than once in the file?  If so, under what conditions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a142246-9d10-4921-8bd4-43eea6838749",
   "metadata": {},
   "source": [
    "HERE PROVIDE EXPLANATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dda2552-bff9-4d05-b7fc-6e72d9526138",
   "metadata": {},
   "source": [
    "**Problem 2.A.6.** Write code to determine if any fields are missing data.  Use isnull() as a first pass.\n",
    "For numeric data, check for zero.  Are there any fields where zero is reasonable?\n",
    "For string categorical data, check for empty strings, \"NA\", \"n/a\", and \"None\" using case-insensitive comparisons.  Did you find any rows suggesting data has been omitted?   If so, how many rows? \n",
    "\n",
    "Whenever performing Exploratory Data Analysis (EDA) we should look to see if there\n",
    "is any missing data.  If so, we either omit those rows or we use *imputation* to \n",
    "infer appropriate values for the missing data.  Either option risks introducing\n",
    "bias, so we must be careful to justify our decisions.  For this problem however, I am\n",
    "only asking for you to identify whether there is missing data and if yes then how \n",
    "many rows are affected in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f065a2-e34f-4086-b736-42e252bc2c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "HERE INSERT A FUNCTION THAT CHECKS FOR ALL OF THE SPECIFIED CONDITIONS AND RETURNS A DATAFRAME \n",
    "COMPRISED OF ROWS THAT ARE AFFLICTED.  OUTPUT SOME OF THE ROWS IF THERE ARE ANY.  OUTPUT HOW MANY\n",
    "ROWS EXHIBIT OMITTED DATA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943d620-5290-4530-9dd3-91588e72ad13",
   "metadata": {},
   "source": [
    "**Problem 2.A.7.** Brazil has 26 states and a federal district.  Are there 27 unique state 2-letter codes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f77dd0a-794c-4e6c-8646-c021cb882096",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSERT CODE TO DETERMINE THE NUMBER OF UNIQUE STATES."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b19d887-1cff-48dd-aca7-c56e222aaa21",
   "metadata": {},
   "source": [
    "**Problem 2.A.8.** Plot the histogram of the number of customers residing in each state and the federal district."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d2f7f6-3f0e-4f5b-9397-3eaacd2ffd5d",
   "metadata": {},
   "source": [
    "HERE INSERT CODE TO CREATE A PLOT USING MATPLOTLIB AND GENERATE THE PLOT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ddff36-b929-4ff8-ae7d-3e4fa878c7c9",
   "metadata": {},
   "source": [
    "**Problem 2.A.9.** There are too many states to get an idea of how the customers are \n",
    "allocated geographically within Brazil.  Add a column to the DataFrame that denotes region\n",
    "that is called `region`.\n",
    "In Brazil states are grouped as follows:\n",
    "\n",
    "* Southeast Region: Espírito Santo, Minas Gerais, Rio de Janeiro, and São Paulo\n",
    "* South Region: Paraná, Santa Catarina, and Rio Grande do Sul\n",
    "* North Region: Acre, Amapá, Amazonas, Pará, Rondônia, Roraima, and Tocantins\n",
    "* Northeast Region: Alagoas, Bahia, Ceará, Maranhão, Paraíba, Pernambuco, Piauí, Rio Grande do Norte, and Sergipe\n",
    "* Central-West Region: Goiás, Mato Grosso, Mato Grosso do Sul, and the Federal District.\n",
    "\n",
    "Are there any customers that do not fall in one of the regions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa78ece-eb0c-48ef-b89a-febf9ef3fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "HERE INSERT CODE TO ADD A ROW THAT CONTAINS THE REGION FOR EACH CUSTOMER."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0233ba-f69f-4d0e-9e7d-dbdd31b36fd4",
   "metadata": {},
   "source": [
    "**Problem 2.A.10.** Plot a histogram of the number of customers in each region.\n",
    "\n",
    "This gives us an idea of the scope of the data and what inferences we might be able to draw\n",
    "regarding regional customer behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8799429-1f90-471f-be7b-c3f0d33a3355",
   "metadata": {},
   "source": [
    "**Problem 2.A.11.** Did adding a region column modify the underlying Spark Dataframe?\n",
    "Describe lineage and the role of lazy evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b126939-7cc2-4305-aec3-d80b6461b579",
   "metadata": {},
   "source": [
    "HERE ADD EXPLANATION."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f163cb2-83b4-456b-9afd-4eb96ca03281",
   "metadata": {},
   "source": [
    "### Part II.B. Geolocation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73730ff-7ae4-44e5-82b9-ecd0dbbe5ca2",
   "metadata": {},
   "source": [
    "Now we will perform some EDA and data cleaning on `olist_geolocation_dataset.csv` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054dde21-4d85-481f-824a-4a991381b9c4",
   "metadata": {},
   "source": [
    "**Problem 2.B.1.** Load the `olist_geolocation_dataset.csv` into a \n",
    "Pandas-on-Spark DataFrame.  Display the first 10 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531ee94c-7abf-4cee-8764-546a9ac3cec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "HERE INSERT CODE AND EXECUTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309f0ba4-1c4d-4c27-b3b5-cd614702e03a",
   "metadata": {},
   "source": [
    "**Problem 2.B.2.** Describe the meaning of the data in each column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c960d41-1f8e-49a1-8191-fb21acbcf2c7",
   "metadata": {},
   "source": [
    "HERE INSERT DESCRIPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0e9c98-25f6-4014-9fa4-a64e55f4e386",
   "metadata": {},
   "source": [
    "**Problem 2.B.3.** Write code to determine if there is any missing data in this DataFrame.\n",
    "Run the code on the DataFrame.  Is there any missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e2a3e1-c3b1-4cb5-8d3d-0443b7f100a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "HERE INSERT CODE AND EXECUTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3369fb63-bb99-44b7-9390-f07b8cc2ccc6",
   "metadata": {},
   "source": [
    "**Problem 2.B.4.** Plot the latitude and longitude of the zip codes on a scatter plot and\n",
    "overlay a bounding box for the min and max latitude and longitude of Brazil with a 1 degree\n",
    "of margin around the westernmost, easternmost, northernmost, and southernmost bounds. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808e7c9d-6cc3-424c-9c42-0803d6f9852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HERE INSERT CODE AND RESULTING PLOT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75257ef9-4f7c-47ac-8630-81e5034a99de",
   "metadata": {},
   "source": [
    "**Problem 2.B.5.** Do any zip codes fall outside the margin surrounding the bounding box\n",
    "containing of Brazil?  If so, write and execute code to count how many fall outside the \n",
    "margin and execute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb978f2e-d490-4eb6-b996-226e1ca80722",
   "metadata": {},
   "source": [
    "HERE INSERT COMMENT. WRITE AND EXECUTE CODE IF APPROPRIATE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1049f939-0d78-41c8-9f28-628b1096348a",
   "metadata": {},
   "source": [
    "**Problem 2.B.6.** Group all zip codes by city-state pair.  How many cities are\n",
    "represented in the data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2477609-e5cf-4b56-976c-484ea27c419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HERE INSERT CODE AND EXECUTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4af0519-f8eb-4a5f-90b3-4d4710ec21fc",
   "metadata": {},
   "source": [
    "**Problem 2.B.7.** Let's try using clustering to clean up the zipcode geolocation\n",
    "data.  For each city-state pair use `scikit-learn`'s DBSCAN to cluster the zip codes\n",
    "based on latitude and longitude.  Try using `eps=5` and `min_samples=5`.  Create a DataFrame \n",
    "with one-row for each city, state pair and that has a column for the centroid of the \n",
    "latitude and longitude of the largest cluster of zip codes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f1bbba-1711-49b7-9a7a-c469790586f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "HERE INSERT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223da318-a691-4cde-9ce9-cc09acc23873",
   "metadata": {},
   "source": [
    "**Problem 2.B.8.** Plot the geolocations of the centroid of the largest clusters.  Are there any city-state pairs \n",
    "where the largest cluster resides outside the margin of the bounding box of Brazil?  If so, \n",
    "what might be done to correct this data?  Are there any city-state pairs that have too few zip codes\n",
    "to form clusters and have zip codes that reside outside the margin of the bouding box of Brazil?\n",
    "Play with values for `eps` and `min_samples` and comment on what happens.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cf2163-fa02-4aa4-9410-ee4c133419ee",
   "metadata": {},
   "source": [
    "**Problem 2.B.9.** Create two new columns in the \n",
    "`olist_geolocation_dataset` DataFrame labelled `corrected_latitude` and \n",
    "`corrected_longitude`.  For any zip code in the geolocation DataFrame\n",
    "that lies within 5 degrees by latitude or longitude of the centroid\n",
    "of the largest cluster of zip codes for that city, state pair, use\n",
    "the original latitude and longitude.  For any zip code in the geolocation\n",
    "DataFrame that lies more than 5 degrees by latitude or longitude from the\n",
    "centroid latitude or longitude of largest cluster, use the cluster\n",
    "latitude and longitude as the correct latitude and longitude for that\n",
    "zip code.  Write and execute the code for to perform these corrections \n",
    "and output the number of zip codes for which cluster latitude and \n",
    "longitude were used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15f7a1b-d888-4e73-adc2-243d855a6d42",
   "metadata": {},
   "source": [
    "**Problem 2.B.10.** Create two scatter plots of latitudes and longitudes.\n",
    "One plot shows all zip codes that use their original latitude and longitude.\n",
    "And a second scatter plot that shows the corrected latitude and longitude\n",
    "using the largest cluster's centroid.  Do any corrected zip codes still\n",
    "fall outside the margin of the bounding box for Brazil?  If so, why?\n",
    "What might be done to fix these zip codes?  If so, try additional methods for \n",
    "data cleaning:\n",
    "\n",
    "1. For city-state pairs that have too few zip codes to form a cluster, use the centroid of\n",
    "the samples within the margin of the bounding box of Brazil.\n",
    "2. For city-state pairs with a single zip code, use each zip-code's geolocation unless\n",
    "it falls outside the margin of Brazil's bounding box, in which case use the centroid of the\n",
    "zip codes with the same state for all such zip codes that land within the margin of\n",
    "Brazil's bounding box.\n",
    "\n",
    "Comment on the success or lack thereof of each strategy in removing outlier geolocations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9234e615-364e-47f2-924d-f7971df626cf",
   "metadata": {},
   "source": [
    "### COMMENT ON EDA AND CLEANING\n",
    "\n",
    "Normally we would continue with this process of EDA and data cleaning for every dataset \n",
    "appearing in the problem.  For purposes of keeping the homework from becoming\n",
    "exorbitantly lengthy, we will stop the cleaning and EDA here and move on to hypothesis tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4848c1-ae2e-465d-9ec2-f64185bd383d",
   "metadata": {},
   "source": [
    "## Part III. Hypothesis Tests\n",
    "\n",
    "I want to perform at least one hypothesis test on the data.\n",
    "\n",
    "**Problem 3.1:** Test for differences in purchase amounts between Rio de Janeiro and \n",
    "Sao Paolo.  \\\\(H_A\\\\): \"The average purchase amount per order in São Paulo is different from that in Rio de Janeiro.\"  Use a t-test.\n",
    "\n",
    "**Problem 3.2:** Repeat problem 3.1 using a permutation test.  Plot a histogram of the permutation differences\n",
    "betwen Sao Paolo and Rio de Janeiro.  Place a vertical line at the observed difference between\n",
    "Rio de Janeiro and Sao Paolo based on the original data.  Compute the p-value.  Does the result deviate from the t-test?\n",
    "\n",
    "**Problem 3.3:** Test for Association Between Review Scores and Delivery Time. Null Hypothesis \\\\(H_0\\\\): There is no correlation between delivery time and review scores.\n",
    "Alternative Hypothesis \\\\(H_A\\\\): There is a negative correlation between delivery time and review scores, indicating that longer delivery times correlate with lower review scores.  Use Pearson correlation.\n",
    "\n",
    "**Problem 3.4** Repeat 3.3 using a permutation test.  Plot a histogram of the distribution of\n",
    "Pearson correlations for each permutation and place a vertical line at the measured correlation \n",
    "for the original correlation between review score and delivery time.\n",
    "Compute the p-value.  Does the result of 3.4 deviate from the result in 3.3?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
